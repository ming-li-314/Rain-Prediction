{"cells":[{"cell_type":"code","source":["# Install torch_geometric\n","!pip install torch_geometric\n"],"metadata":{"id":"zAnyK8xMg8b4"},"id":"zAnyK8xMg8b4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import libraries\n","import torch\n","import xarray as xr\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","from torch_geometric.data import Data\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","import logging\n","from tqdm.notebook import tqdm  # Use notebook-specific tqdm for single-line updates\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"use1cXaglAZV"},"id":"use1cXaglAZV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the NetCDF file\n","nc_file1 = \"/content/drive/MyDrive/ERA5_datasets/data_accum.nc\"\n","nc_file2 = \"/content/drive/MyDrive/ERA5_datasets/data_instant.nc\"\n","ds1 = xr.open_dataset(nc_file1)\n","ds2 = xr.open_dataset(nc_file2)\n","\n","ds = xr.merge([ds1, ds2], compat='override')\n","# Print dataset details\n","print(ds)\n","\n","# Select variables\n","variables = ['tp', 't2m', 'd2m', 'u10', 'v10', 'msl', 'tcwv', 'cape', 'slhf', 'sshf']\n","data_arrays = [ds[var].values for var in variables]\n","\n","# Stack variables into tensor [time, lat, lon, channels]\n","data = np.stack(data_arrays, axis=-1)  # Shape: [hours, lat, lon, 10]\n","\n","# Normalize using StandardScaler (fit on train only)\n","hours = ds.valid_time.values\n","total_hours = len(hours)\n","train_end = int(0.8 * total_hours)\n","val_end = train_end + int(0.1 * total_hours)\n","scaler = StandardScaler()\n","train_data_reshaped = data[:train_end].reshape(-1, data.shape[-1])\n","scaler.fit(train_data_reshaped)\n","# Normalize in-place to save memory\n","data = scaler.transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n","\n","# Store tp for labels\n","tp = ds['tp'].values\n","\n","# Define dataset class\n","class ERA5Dataset(Dataset):\n","    def __init__(self, data, tp, indices, input_window=12, target_window=6, threshold=0.001):\n","        self.data = data\n","        self.tp = tp\n","        self.indices = indices\n","        self.input_window = input_window\n","        self.target_window = target_window\n","        self.threshold = threshold\n","        self.grid_size = (data.shape[1], data.shape[2])\n","\n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def __getitem__(self, idx):\n","        t = self.indices[idx]\n","        input_data = self.data[t:t+self.input_window]\n","        future_tp = self.tp[t+self.input_window:t+self.input_window+self.target_window]\n","        rain_label = (future_tp.sum(axis=0) > self.threshold).astype(np.int64)\n","        if not np.all(np.isin(rain_label, [0, 1])):\n","            print(f\"Warning: Non-binary labels detected at index {idx}: {np.unique(rain_label)}\")\n","            rain_label = np.clip(rain_label, 0, 1)\n","        return (torch.tensor(input_data, dtype=torch.float32),\n","                torch.tensor(rain_label, dtype=torch.long))\n","\n","# Create indices for each split\n","input_window = 12\n","target_window = 6\n","train_indices = list(range(0, train_end - input_window - target_window + 1))\n","val_indices = list(range(train_end, val_end - input_window - target_window + 1))\n","test_indices = list(range(val_end, total_hours - input_window - target_window + 1))\n","\n","# Initialize datasets\n","train_dataset = ERA5Dataset(data, tp, train_indices, input_window, target_window)\n","val_dataset = ERA5Dataset(data, tp, val_indices, input_window, target_window)\n","test_dataset = ERA5Dataset(data, tp, test_indices, input_window, target_window)\n","\n","# Create data loaders\n","batch_size = 8\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Compute rain percentage (sample-based to save memory)\n","rain_count = 0\n","total_nodes = 0\n","for idx in train_indices[:100]:\n","    future_tp = tp[idx+input_window:idx+input_window+target_window]\n","    rain_label = (future_tp.sum(axis=0) > 0.001).astype(int)\n","    rain_count += rain_label.sum()\n","    total_nodes += rain_label.size\n","rain_percentage = (rain_count / total_nodes) * 100 if total_nodes > 0 else 0\n","print(f\"Approximate percentage of rain in training dataset: {rain_percentage:.2f}%\")"],"metadata":{"id":"9kGD9WNylDJ8"},"id":"9kGD9WNylDJ8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Updated CNNRainPredictor with third Conv3d layer\n","class CNNRainPredictor(nn.Module):\n","    def __init__(self, input_channels, hidden_dim, output_dim, grid_size):\n","        super(CNNRainPredictor, self).__init__()\n","        self.grid_size = grid_size\n","        self.conv1 = nn.Conv3d(input_channels, hidden_dim, kernel_size=(3, 3, 3), padding=1)\n","        self.conv2 = nn.Conv3d(hidden_dim, hidden_dim, kernel_size=(3, 3, 3), padding=1)\n","        self.conv3 = nn.Conv3d(hidden_dim, hidden_dim, kernel_size=(3, 3, 3), padding=1)  # New layer\n","        self.fc = nn.Conv2d(hidden_dim, output_dim, kernel_size=1)\n","        self.dropout = nn.Dropout(0.4)\n","        self.pool = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n","        self.upsample = nn.Upsample(size=grid_size, mode='bilinear', align_corners=False)\n","\n","    def forward(self, x):\n","        x = x.permute(0, 4, 1, 2, 3)  # (batch_size, T, H, W, C) -> (batch_size, C, T, H, W)\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.pool(x)\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = self.conv3(x)\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = x.mean(dim=2)\n","        x = self.fc(x)\n","        x = self.upsample(x)\n","        return x\n","\n","input_channels = 10\n","hidden_dim = 128\n","output_dim = 2\n","grid_size = (data.shape[1], data.shape[2])\n","model = CNNRainPredictor(input_channels, hidden_dim, output_dim, grid_size).to(device)\n"],"metadata":{"id":"7F0r6lq6lDNU"},"id":"7F0r6lq6lDNU","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Define loss functions and training utilities\n","def soft_f1_loss(y_true, y_pred, epsilon=1e-6):\n","    y_true = y_true.float()\n","    y_pred = torch.softmax(y_pred, dim=1)[:, 1, :, :]\n","    tp = torch.sum(y_true * y_pred)\n","    fp = torch.sum((1 - y_true) * y_pred)\n","    fn = torch.sum(y_true * (1 - y_pred))\n","    soft_precision = tp / (tp + fp + epsilon)\n","    soft_recall = tp / (tp + fn + epsilon)\n","    soft_f1 = 2 * (soft_precision * soft_recall) / (soft_precision + soft_recall + epsilon)\n","    return 1 - soft_f1\n","\n","def focal_loss(y_pred, y_true, gamma=2.0, alpha=None):\n","    ce_loss = F.cross_entropy(y_pred, y_true, weight=alpha, reduction='none')\n","    p_t = torch.exp(-ce_loss)\n","    focal_loss = (1 - p_t) ** gamma * ce_loss\n","    return focal_loss.mean()\n","\n","criterion_ce = nn.CrossEntropyLoss()\n","logging.basicConfig(filename='training_log.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n","logger = logging.getLogger()\n","train_losses = []\n","train_accuracies = []\n","val_f1_scores = []\n","test_f1_scores = []\n","\n","# Modified training cell with threshold grid search, fixed class weight, and optimal threshold for test\n","def train(model, train_loader, optimizer, device, class_weights, alpha=0.5, accum_steps=4):\n","    model.train()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","    optimizer.zero_grad()\n","    # Tqdm progress bar for training batches\n","    for i, (data, target) in enumerate(tqdm(train_loader, desc=\"Training Batches\", leave=False)):\n","        data, target = data.to(device), target.to(device)\n","        out = model(data)\n","        if out.shape[2:] != target.shape[1:]:\n","            raise ValueError(f\"Output spatial shape {out.shape[2:]} doesn't match target {target.shape[1:]}\")\n","        ce_loss = criterion_ce(out, target)\n","        f1_loss = soft_f1_loss(target, out)\n","        focal = focal_loss(out, target, alpha=class_weights.to(device))\n","        loss = alpha * ce_loss + (1 - alpha) * f1_loss + focal\n","        loss = loss / accum_steps\n","        loss.backward()\n","        if (i + 1) % accum_steps == 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        total_loss += loss.item() * accum_steps\n","        preds = out.argmax(dim=1)\n","        correct += (preds == target).sum().item()\n","        total += target.numel()\n","    if len(train_loader) % accum_steps != 0:\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        optimizer.zero_grad()\n","    avg_loss = total_loss / len(train_loader)\n","    accuracy = correct / total\n","    return avg_loss, accuracy\n","\n","def evaluate(model, loader, device, threshold=0.77, dataset_name=\"Test\"):\n","    model.eval()\n","    all_probs = []\n","    all_labels = []\n","    total_loss = 0\n","    total_samples = 0\n","    expected_samples = len(loader.dataset)\n","\n","    with torch.no_grad():\n","        for data, target in tqdm(loader, desc=f\"{dataset_name} Batches\", leave=False):\n","            data, target = data.to(device), target.to(device)\n","            out = model(data)\n","            loss = criterion_ce(out, target)\n","            total_loss += loss.item() * data.size(0)\n","\n","            # Extract probabilities for rain class\n","            probs = torch.softmax(out, dim=1)[:, 1, :, :]  # Shape: [batch_size, lat, lon]\n","            all_probs.append(probs.cpu().numpy())\n","            all_labels.append(target.cpu().numpy())\n","            total_samples += data.size(0)\n","\n","    # Verify total samples\n","    if total_samples != expected_samples:\n","        print(f\"Warning: Processed {total_samples} samples, expected {expected_samples}\")\n","\n","    # Concatenate across batches\n","    all_probs = np.concatenate(all_probs, axis=0)  # Shape: [num_samples, lat, lon]\n","    all_labels = np.concatenate(all_labels, axis=0)  # Shape: [num_samples, lat, lon]\n","\n","    # Check shape consistency\n","    if all_probs.shape != all_labels.shape:\n","        raise ValueError(f\"Shape mismatch: all_probs {all_probs.shape}, all_labels {all_labels.shape}\")\n","\n","    avg_loss = total_loss / total_samples if total_samples > 0 else float('inf')\n","\n","    # Compute predictions with explicit thresholding\n","    preds = (all_probs >= threshold).astype(np.int64)\n","    # Verify preds shape\n","    if preds.shape != all_labels.shape:\n","        raise ValueError(f\"Preds shape {preds.shape} does not match all_labels shape {all_labels.shape}\")\n","\n","    # Compute metrics\n","    f1 = f1_score(all_labels.flatten(), preds.flatten(), average='binary')\n","    accuracy = accuracy_score(all_labels.flatten(), preds.flatten())\n","    precision = precision_score(all_labels.flatten(), preds.flatten())\n","    recall = recall_score(all_labels.flatten(), preds.flatten())\n","    pred_rain = np.sum(preds)\n","    true_rain = np.sum(all_labels)\n","\n","    print(f\"{dataset_name} - Threshold: {threshold:.2f}, Loss: {avg_loss:.4f}, F1: {f1:.4f}, \"\n","          f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n","    print(f\"{dataset_name} - Predicted Rain Nodes: {pred_rain}, True Rain Nodes: {true_rain}\")\n","\n","    return f1, accuracy, precision, recall, avg_loss, all_probs, all_labels"],"metadata":{"id":"mdurTtsWlDQr"},"id":"mdurTtsWlDQr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","\n","# Training with fixed class weight and threshold\n","class_weight = [1.0, 8.0]\n","fixed_threshold = 0.77\n","num_epochs = 20\n","best_val_f1 = 0.0\n","\n","print(f\"Starting Training with Class Weight: {class_weight} and Fixed Threshold: {fixed_threshold:.2f}\")\n","model = CNNRainPredictor(input_channels, hidden_dim, output_dim, grid_size).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=3e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n","class_weights = torch.tensor(class_weight, dtype=torch.float32).to(device)\n","patience = 6\n","patience_counter = 0\n","\n","for epoch in tqdm(range(1, num_epochs + 1), desc=\"Epochs\"):\n","    train_loss, train_acc = train(model, train_loader, optimizer, device, class_weights, alpha=0.5, accum_steps=4)\n","    train_losses.append(train_loss)\n","    train_accuracies.append(train_acc)\n","    val_f1, val_acc, val_prec, val_rec, val_loss, _, _ = evaluate(model, val_loader, device,\n","                                                                 threshold=fixed_threshold, dataset_name=\"Validation\")\n","    val_f1_scores.append(val_f1)\n","    test_f1, test_acc, test_prec, test_rec, test_loss, _, _ = evaluate(model, test_loader, device,\n","                                                                      threshold=fixed_threshold, dataset_name=\"Test\")\n","    test_f1_scores.append(test_f1)\n","    log_msg = (f'Class Weight: {class_weight}, Threshold: {fixed_threshold:.2f}, Epoch {epoch:02d}, '\n","               f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val F1: {val_f1:.4f}, Test F1: {test_f1:.4f}')\n","    logger.info(log_msg)\n","    print(log_msg)\n","    scheduler.step(val_f1)\n","\n","    if val_f1 > best_val_f1:\n","        best_val_f1 = val_f1\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/best_CNN_model.pth')\n","        patience_counter = 0\n","        print(f\"New best model saved with Val F1: {val_f1:.4f}\")\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","print(f\"\\nFinal Evaluation of Best Model with Fixed Threshold: {fixed_threshold:.2f}\")\n","model.load_state_dict(torch.load('/content/drive/MyDrive/best_CNN_model.pth'))\n","test_f1, test_acc, test_prec, test_rec, test_loss, all_probs, all_labels = evaluate(\n","    model, test_loader, device, threshold=fixed_threshold, dataset_name=\"Test\")"],"metadata":{"id":"OrT8YnJ7lLA0"},"id":"OrT8YnJ7lLA0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install cartopy"],"metadata":{"id":"XeXdnfAxlLDt"},"id":"XeXdnfAxlLDt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, roc_auc_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import cartopy.crs as ccrs\n","import cartopy.feature as cfeature\n","from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n","import matplotlib.patches as mpatches"],"metadata":{"id":"7fazhjdglLGp"},"id":"7fazhjdglLGp","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify shapes before confusion matrix\n","print(f\"Confusion Matrix Inputs - all_labels shape: {all_labels.shape}, all_probs shape: {all_probs.shape}\")\n","if all_labels.shape != all_probs.shape:\n","    raise ValueError(f\"Shape mismatch: all_labels {all_labels.shape}, all_probs {all_probs.shape}\")\n","\n","# Compute predictions with explicit thresholding\n","preds = (all_probs >= fixed_threshold).astype(np.int64)\n","print(f\"Preds shape after thresholding: {preds.shape}\")\n","\n","# Verify flattened shapes\n","flat_labels = all_labels.flatten()\n","flat_preds = preds.flatten()\n","print(f\"Flattened shapes - labels: {flat_labels.shape}, preds: {flat_preds.shape}\")\n","if flat_labels.shape != flat_preds.shape:\n","    raise ValueError(f\"Flattened shape mismatch: labels {flat_labels.shape}, preds {flat_preds.shape}\")\n","\n","cm = confusion_matrix(flat_labels, flat_preds)\n","cm_percent = cm / cm.sum(axis=1, keepdims=True) * 100  # Normalize by row\n","sns.set(font_scale=1.5)\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm_percent, annot=True, fmt='.2f', cmap='Blues', cbar=False,\n","            xticklabels=['No Rain', 'Rain'], yticklabels=['No Rain', 'Rain'],\n","            annot_kws={'size': 15})\n","for text in plt.gca().texts:\n","    text.set_text(f\"{float(text.get_text()):.2f}%\")\n","plt.title('Confusion Matrix (CNN_Model)', fontsize=22)\n","plt.xlabel('Predicted', fontsize = 20)\n","plt.ylabel('True', fontsize = 20)\n","plt.savefig('/content/drive/MyDrive/CNN_confusion_matrix_row_normalized.png')\n","plt.show()"],"metadata":{"id":"SBdQf0rzlLJ9"},"id":"SBdQf0rzlLJ9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute ROC curve\n","fpr, tpr, _ = roc_curve(flat_labels, all_probs.flatten())\n","roc_auc = roc_auc_score(flat_labels, all_probs.flatten())\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc='lower right')\n","plt.savefig('/content/drive/MyDrive/CNN_roc_curve.png')\n","plt.show()\n","\n","# Compute Precision-Recall curve\n","precision, recall, _ = precision_recall_curve(flat_labels, all_probs.flatten())\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, label='Precision-Recall Curve')\n","plt.xlabel('Recall', fontsize=20)\n","plt.ylabel('Precision', fontsize=20)\n","plt.title('Precision-Recall Curve (CNN Model)', fontsize=22)\n","plt.legend(loc='lower left')\n","plt.savefig('/content/drive/MyDrive/CNN_precision_recall_curve.png')\n","plt.show()"],"metadata":{"id":"HvZGXnBQlLND"},"id":"HvZGXnBQlLND","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot F1 score vs. epochs\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(1, len(val_f1_scores) + 1), val_f1_scores, label='Validation F1')\n","plt.plot(range(1, len(test_f1_scores) + 1), test_f1_scores, label='Test F1')\n","plt.xlabel('Epoch')\n","plt.ylabel('F1 Score')\n","plt.title('F1 Score vs. Epoch')\n","plt.legend()\n","plt.grid(True)\n","plt.savefig('/content/drive/MyDrive/CNN_f1_score_vs_epoch.png')\n","plt.show()"],"metadata":{"id":"_DVKza5FlXeX"},"id":"_DVKza5FlXeX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Spatial Map Comparison for Selected Test Samples\n","# Define latitude and longitude\n","lat = ds.latitude.values  # Shape: (105,)\n","lon = ds.longitude.values  # Shape: (237,)\n","\n","# Debug coordinate shapes and ranges\n","#print(f\"Longitude shape: {lon.shape}, range: {lon.min():.2f} to {lon.max():.2f}\")\n","#print(f\"Latitude shape: {lat.shape}, range: {lat.min():.2f} to {lat.max():.2f}\")\n","\n","test_sample_indices = [0, 50, 100]  # Valid indices for len(test_dataset) = 199\n","\n","for test_sample_idx in test_sample_indices:\n","    if test_sample_idx >= len(test_dataset):\n","        print(f\"Skipping index {test_sample_idx}: Out of bounds (max {len(test_dataset)-1})\")\n","        continue\n","    input_data, true_labels = test_dataset[test_sample_idx]  # Unpack input and labels\n","    input_data = input_data.unsqueeze(0).to(device)  # Add batch dimension\n","    with torch.no_grad():\n","        model.eval()\n","        out = model(input_data)\n","        probs = torch.softmax(out, dim=1)[:, 1, :, :]  # Shape: [batch_size, N_lat, N_lon]\n","        preds = (probs >= fixed_threshold).float()\n","    preds = preds.cpu().numpy().squeeze()  # Shape: [105, 237]\n","    true = true_labels.cpu().numpy()  # Shape: [105, 237]\n","\n","    # Debug shapes\n","    #print(f\"Sample {test_sample_idx}: true shape: {true.shape}, preds shape: {preds.shape}\")\n","\n","    # Get indices where rain occurs\n","    true_rain_idx = np.where(true == 1)\n","    pred_rain_idx = np.where(preds == 1)\n","\n","    # Debug index ranges\n","    #print(f\"Sample {test_sample_idx}: True rain row indices max: {true_rain_idx[0].max() if len(true_rain_idx[0]) > 0 else 'None'}\")\n","    #print(f\"Sample {test_sample_idx}: True rain col indices max: {true_rain_idx[1].max() if len(true_rain_idx[1]) > 0 else 'None'}\")\n","\n","    # Map indices to coordinates\n","    true_lat = lat[true_rain_idx[0]]  # Rows (latitude, size 105)\n","    true_lon = lon[true_rain_idx[1]]  # Columns (longitude, size 237)\n","    pred_lat = lat[pred_rain_idx[0]]\n","    pred_lon = lon[pred_rain_idx[1]]\n","\n","    # Debug number of points and coordinate ranges\n","    print(f\"Sample {test_sample_idx}: True rain points: {len(true_lon)}, \")\n","    #      f\"lon range: {true_lon.min():.2f} to {true_lon.max():.2f}, \"\n","    #      f\"lat range: {true_lat.min():.2f} to {true_lat.max():.2f}\")\n","    print(f\"Sample {test_sample_idx}: Predicted rain points: {len(pred_lon)}, \")\n","    #      f\"lon range: {pred_lon.min():.2f} to {pred_lon.max():.2f}, \"\n","    #      f\"lat range: {pred_lat.min():.2f} to {pred_lat.max():.2f}\")\n","    #\n","    # Spatial Map\n","    fig = plt.figure(figsize=(12, 8))\n","    ax = plt.axes(projection=ccrs.PlateCarree())\n","    ax.set_extent([-125, -65, 24, 50], crs=ccrs.PlateCarree())\n","    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n","    ax.add_feature(cfeature.OCEAN)\n","    ax.add_feature(cfeature.COASTLINE)\n","    ax.add_feature(cfeature.BORDERS)\n","    ax.add_feature(cfeature.STATES, linestyle='--')\n","\n","    # Scatter plots\n","    ax.scatter(true_lon, true_lat, c='red', s=50, alpha=0.5, label='True Rain', transform=ccrs.PlateCarree())\n","    ax.scatter(pred_lon, pred_lat, c='blue', s=50, alpha=0.5, label='Predicted Rain', transform=ccrs.PlateCarree())\n","\n","    plt.title(f'True vs. Predicted Rain (Test Sample {test_sample_idx}, CNN Model)')\n","    plt.legend()\n","    plt.savefig(f'/content/drive/MyDrive/CNN_spatial_prediction_{test_sample_idx}.png')\n","    plt.show()"],"metadata":{"id":"DxpSDrpulXhM"},"id":"DxpSDrpulXhM","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MRfHIPPjlXju"},"id":"MRfHIPPjlXju","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jIkmAEf5lXmI"},"id":"jIkmAEf5lXmI","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wWdB8gbLlXoc"},"id":"wWdB8gbLlXoc","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"z-4wHxKSlXqr"},"id":"z-4wHxKSlXqr","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}